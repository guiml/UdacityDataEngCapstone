{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is the capstone project for the Udacity's Data Engineering Nanodegree course.\n",
    "\n",
    "I chose to execute the suggested project by Udacity which is composed of 4 data sources provided in the course's platform:\n",
    "\n",
    "| Dataset                               | Format                 |  Fields # | Aprox Record Count |\n",
    "| :---                                  |    :----:              | :----:    |  :----:            |\n",
    "| I94 Arrivals and Departures in the US | SAS BDAT               | 28        |  40 million        |\n",
    "| Global Temperatures (Cities)          | CSV                    | 07        |   8 million        |\n",
    "| Cities Demographics                   | JSON                   | 13        |  3 thousand        |\n",
    "| Airport Codes                         | CSV                    | 12        |  57 thousand       |\n",
    "\n",
    "## Project understanding outline\n",
    "\n",
    "In order to aproximate this capstone project to a real-life project, I developed a hypothetical scenario where I was being provided with the aforementioned datasets and I was tasked to propose an analytical Data Structure (OLAP) to facilitate the analysis of this information by a team of Business Analysts that needed to read this from an AWS RedShift instance. \n",
    "\n",
    "### About the datasets\n",
    "\n",
    "The datasets are comprised of information on immigration process into the United States. The base dataset is the I94 records that are comprised with the information on each passenger that arrived or departed from the United States throughout a year. For that reason I decided to **filter only information related to the United States**.\n",
    "\n",
    "### Proposed Pipeline\n",
    "\n",
    "With that purpose in mind I planned the following Data Pipeline:\n",
    "\n",
    "1) Data Sets in file format are ingested in a S3 bucket\n",
    "2) Due to the data sizes , a Spark Cluster instance (AWS EMR) imports the data and process the files \n",
    "3) The Spark Cluster write the transformed data into tables in the RedShift environment\n",
    "4) The team can then access the RedShift environment and query the data easily\n",
    "\n",
    "### Proposed Star Data Schema for the solution\n",
    "\n",
    "<img src=\"./StarSchema.png\"/>\n",
    "\n",
    "## Steps of the project\n",
    "\n",
    "### Preparation\n",
    "\n",
    "*For this project I had an AWS Environment already set with a CLI configured with AWS Secret and Key. As well a user with permissions to manipulate S3, EMR and Redshift.\n",
    "\n",
    "#### Download datasets\n",
    "\n",
    "1) Retrieve i94 dataset from Udacity Environment:\n",
    "    > zip -r data.zip ../../data/18-83510-I94-Data-2016\n",
    "2) Downloaded I94 Immigration Data (Data.zip)\n",
    "3) Downloaded [GlobalLandTemperatures](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "4) Downloaded [US City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "5) Downloaded [Airport Codes](https://datahub.io/core/airport-codes/r/airport-codes.csv)\n",
    "\n",
    "#### Upload the data to S3\n",
    "\n",
    "Uploaded all the datasets into a newly created S3 bucket created just for this project.\n",
    "\n",
    "#### Preparation of the EMR Cluster\n",
    "\n",
    "Created an EMR cluster with the most up-to-date version, making sure I associate the PEM key to this cluster for management via CLI. Configured Security Groups to accept external connections via SSH in the Inbound Rules.\n",
    "\n",
    "#### Preparation of the Redshift Cluster\n",
    "\n",
    "Created an Redshift cluster. Made public access available.\n",
    "\n",
    "#### Code preparation\n",
    "\n",
    "I installed a local environment to run Data Quality and prepare the ETL with pure python code. I created two local Jupyter Notebooks:\n",
    "1) data_quality.ipynb \n",
    "2) etl.ipynb\n",
    "\n",
    "### Execution\n",
    "\n",
    "#### ETL Execution\n",
    "\n",
    "To execute the ETL, it is needed to: \n",
    "\n",
    "1) In the terminal window, access the EMR Instance\n",
    "    > aws % ssh -i KEY.pem hadoop@EMRENDPOINTADDRESS\n",
    "2) Upload etl python file to EMR environment\n",
    "    > scp etl.py hadoop@EMRENDPOINTADDRESS\n",
    "3) Run the etl script*\n",
    "    > /usr/bin/spark-submit --packages saurfang:spark-sas7bdat:3.0.0-s_2.12 --master yarn etl.py\n",
    "\n",
    "*This commnand ensures we are using the SAS BDAT package to read the SAS files\n",
    "\n",
    "### Data Analysis\n",
    "\n",
    "Using any SQL client, the analyst can connect to the Redshift instance and runs some commands, here are some examples\n",
    "\n",
    "*Count the cities with the higher number of immigration activity*\n",
    "\n",
    "    SELECT\n",
    "        cityname,\n",
    "        COUNT(cicid)\n",
    "        FROM\n",
    "        arrivals\n",
    "    GROUP BY\n",
    "        cityname\n",
    "    ORDER BY\n",
    "        COUNT(cicid) DESC\n",
    "\n",
    "\n",
    "*Check the airlines used by the visitants from Brazil*\n",
    "\n",
    "    SELECT DISTINCT\n",
    "        a.airline,\n",
    "        i.res_country\n",
    "    FROM\n",
    "        arrivals a\n",
    "        JOIN immigrants i ON a.cicid = i.cicid\n",
    "    WHERE\n",
    "        i.res_country = 'Brazil';\n",
    "\n",
    "### Scalability\n",
    "\n",
    "The usage of a Big Data Spark environment was made to address the issue of scaliability. If the data is increased by 100 times, we could configure the EMR environment to use more clusters and or more powerful machines.\n",
    "\n",
    "In order to schedule regular runnings of the pipeline, we could develop an Apache Airflow to schedule the running of this service.\n",
    "\n",
    "The usage of Redshift was made to accomodate scalable access by the users as well.\n",
    "\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "*Table Arrivals:*\n",
    "\n",
    "| Field| Description|\n",
    "| :--- |    :----:  |\n",
    "| cicid| Passenger Identifier|\n",
    "| airline| Airline used by the passenger|\n",
    "| i94port  | Port of entry in the US|\n",
    "| depdate  | Departure Date|\n",
    "| arrdate  | Arrival Date|\n",
    "| city  | City of port of entry|\n",
    "| cityname  | Normalized City name|\n",
    "\n",
    "*Table Immigrations:*\n",
    "\n",
    "| Field| Description|\n",
    "| :--- |    :----:  |\n",
    "| cicid| Passenger Identifier|\n",
    "| i94visa| Visa code |\n",
    "| visatype  | Type of visa used for entry/departure|\n",
    "| cit_country  | Country of citizenship of the traveler |\n",
    "| res_country  | Country of residency of the traveler |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Table Personal Demographics:*\n",
    "\n",
    "| Field| Description|\n",
    "| :--- |    :----:  |\n",
    "| cicid| Passenger Identifier|\n",
    "| biryear| Birthday year of the traveler |\n",
    "| gender | Gender of the traveler|\n",
    "\n",
    "*Table Cities Demographics:*\n",
    "\n",
    "| Field| Description|\n",
    "| :--- |    :----:  |\n",
    "| average_household_size| Average household size | \n",
    "| city| City Name | \n",
    "| count| Count of the  race in the city | \n",
    "| female_population| Count of females in the city | \n",
    "| foreign_born| Count of foreign_born in the city | \n",
    "| male_population| Count of males in the city | \n",
    "| median_age| Median age in the city | \n",
    "| number_of_veterans| Count of veterans in the city | \n",
    "| race| Count of females in the city | \n",
    "| state| State of the city | \n",
    "| total_population| City's total population | \n",
    "\n",
    "*Table Cities Temperatures:*\n",
    "\n",
    "| Field| Description|\n",
    "| :--- |    :----:  |\n",
    "| mintemperature | Minimun average temperature |\n",
    "| maxtemperature | Maximum average temperature |\n",
    "| avgtemperature | Mean average temperature |\n",
    "| city | City name |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
